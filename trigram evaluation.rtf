{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1033{\fonttbl{\f0\fnil\fcharset0 Times New Roman;}{\f1\fnil\fcharset1 Cambria Math;}}
{\*\generator Riched20 10.0.19041}{\*\mmathPr\mmathFont1\mwrapIndent1440 }\viewkind4\uc1 
\pard\sa200\sl276\slmult1\b\f0\fs36\lang9                             Trigram Language Model \par
\b0\fs24\par
\b\fs28 Overview\par
\b0\fs24 This document outlines the key design decisions made in implementing an optimized trigram language model with focus on time/space complexity and code efficiency.\par
\par
\b Core Design Decisions\par
\b0 1. Data Structure Selection\par
\b python\b0\par
self.trigram_counts = defaultdict(Counter)  # \{(w1, w2): \{w3: count\}\}\par
self.context_totals = defaultdict(int)       # \{(w1, w2): total\}\par
\par
Rationale\par

\pard 
{\pntext\f0 1.\tab}{\*\pn\pnlvlbody\pnf0\pnindent0\pnstart1\pndec{\pntxta.}}
\fi-360\li720\sa200\sl276\slmult1 - Time Complexity: O(1) average case for insertions and lookups\par
{\pntext\f0 2.\tab}- Space Efficiency: Only stores observed trigrams (no sparse matrix)\par
{\pntext\f0 3.\tab}- Memory Overhead: Counter provides efficient count updates\par
{\pntext\f0 4.\tab}- Alternative Considered: Nested regular dicts require explicit key checking, adding overhead\par

\pard\sa200\sl276\slmult1 Impact: For a typical book (~100K words), this reduces memory from O(V\'b3) where V is vocabulary size to O(T) where T is unique trigrams (~10-50K), achieving ~99% space reduction.\par
\par
\b 2. Vocabulary Management\b0\par
 Frequency-based vocabulary with UNK token\par
\b python\par
\b0 self.min_word_freq = 2\par
self.vocab = \{word for word, count in word_freq.items() if count >= min_word_freq\}\par
Rationale:\par

\pard 
{\pntext\f0 1.\tab}{\*\pn\pnlvlbody\pnf0\pnindent0\pnstart1\pndec{\pntxta.}}
\fi-360\li720\sa200\sl276\slmult1  Reduces Sparsity: Rare words (appearing once) are replaced with `<UNK>`\par
{\pntext\f0 2.\tab} Generalizes Better:Model learns patterns for unknown words\par
{\pntext\f0 3.\tab} Space Savings:Typical vocabulary reduction of 30-50%\par
{\pntext\f0 4.\tab} Trade-off: Slight information loss vs. significant memory/performance gain\par
{\pntext\f0 5.\tab}\lang9 Metrics: For "Alice in Wonderland": ~5,000 unique words \f1\u8594?\f0  ~2,500 vocabulary size\par

\pard\sa200\sl276\slmult1\par
\b 3. Probabilistic Sampling Algorithm\par
\b0  Cumulative distribution with binary search\par
\b python\par
\b0 cumulative = []\par
for word in words:\par
    cumsum += next_words[word] / total\par
    cumulative.append(cumsum)\par
idx = bisect.bisect_left(cumulative, random.random())\par
\par
Rationale:\par

\pard 
{\pntext\f0 1.\tab}{\*\pn\pnlvlbody\pnf0\pnindent0\pnstart1\pndec{\pntxta.}}
\fi-360\li720\sa200\sl276\slmult1  Time Complexity:O(m log m) where m is next word candidates\par
{\pntext\f0 2.\tab} Correct Sampling: Uses true probability distribution (not argmax)\par
{\pntext\f0 3.\tab} Alternative Rejected:`random.choices()` is simpler but creates full distribution array\par
{\pntext\f0 4.\tab} Performance: Binary search is faster for m > 10 candidates\par

\pard\sa200\sl276\slmult1\par
\b  4. Text Preprocessing Pipeline\par
\b0  Multi-stage cleaning with regex\par

\pard 
{\pntext\f0 1.\tab}{\*\pn\pnlvlbody\pnf0\pnindent0\pnstart1\pndec{\pntxta.}}
\fi-360\li720\sa200\sl276\slmult1  Lowercase normalization\par
{\pntext\f0 2.\tab} Whitespace normalization\par
{\pntext\f0 3.\tab} Selective punctuation retention\par
{\pntext\f0 4.\tab} Sentence boundary detection\par

\pard\sa200\sl276\slmult1\par
Rationale:\par

\pard 
{\pntext\f0 1.\tab}{\*\pn\pnlvlbody\pnf0\pnindent0\pnstart1\pndec{\pntxta.}}
\fi-360\li720\sa200\sl276\slmult1 Balance: Preserves sentence structure while reducing noise\par
{\pntext\f0 2.\tab}Efficiency:Single-pass regex operations are O(n)\par
{\pntext\f0 3.\tab}Punctuation: Kept `.!?` for sentence boundaries, essential for START/END tokens\par
{\pntext\f0 4.\tab}Trade-off: Loses some stylistic information for cleaner patterns\par

\pard\sa200\sl276\slmult1\par
\b 5. Padding Strategy\par
\b0\par
 Double START tokens, END at sentence boundaries\par
\b python\par
\b0 [START, START, word1, word2, ..., wordN, END, START, START, ...]\par
\par
Rationale:\par

\pard 
{\pntext\f0 1.\tab}{\*\pn\pnlvlbody\pnf0\pnindent0\pnstart1\pndec{\pntxta.}}
\fi-360\li720\sa200\sl276\slmult1 Trigram Completeness: Ensures first word has valid (START, START, word1) trigram\par
{\pntext\f0 2.\tab}Sentence Awareness: END/START pairs preserve sentence boundaries\par
{\pntext\f0 3.\tab}Generation:Natural stopping condition when END is sampled\par
{\pntext\f0 4.\tab}Standard Practice: Matches n-gram literature conventions\par

\pard\sa200\sl276\slmult1\par
\b Complexity Analysis\par
\b0\par
\b  Training (fit method)\par

\pard 
{\pntext\f0 1.\tab}{\*\pn\pnlvlbody\pnf0\pnindent0\pnstart1\pndec{\pntxta.}}
\fi-360\li720\sa200\sl276\slmult1\b0 Time: O(n) where n = word count\par
{\pntext\f0 2.\tab} Cleaning: O(n)\par
{\pntext\f0 3.\tab} Tokenization: O(n)\par
{\pntext\f0 4.\tab} Trigram counting: O(n) with O(1) dict operations\par
{\pntext\f0 5.\tab} Space:O(T + V) where T = unique trigrams, V = vocabulary\par
{\pntext\f0 6.\tab} Trigrams: ~10K-50K for typical book\par
{\pntext\f0 7.\tab} Vocabulary: ~2K-5K words\par

\pard\sa200\sl276\slmult1\par
\b Generation (generate method)\par

\pard 
{\pntext\f0 1.\tab}{\*\pn\pnlvlbody\pnf0\pnindent0\pnstart1\pndec{\pntxta.}}
\fi-360\li720\sa200\sl276\slmult1\b0 Time:O(k \'d7 m log m) where k = generation length, m = avg candidates\par
{\pntext\f0 2.\tab}Per word: O(m log m) for sampling\par
{\pntext\f0 3.\tab}Typical m: 5-20 words\par
{\pntext\f0 4.\tab}Space: O(k) for generated sequence\par

\pard\sa200\sl276\slmult1\par
\b Comparison with Alternatives\par
\b0\par
| Approach | Space | Time (train) | Time (generate) |\par
|----------|-------|--------------|-----------------|\par
| Our solution | O(T) | O(n) | O(k\'d7m log m) |\par
| Full probability matrix | O(V\'b3) | O(n+V\'b3) | O(k) |\par
| Trie structure | O(T\'d7L) | O(n\'d7L) | O(k\'d7L) |\par
\par
Where L = average word length. Our solution provides best space efficiency.\par
\par
\b  Implementation Optimizations\par
\b0\par
 1. Avoid Redundant Calculations\par

\pard 
{\pntext\f0 1.\tab}{\*\pn\pnlvlbody\pnf0\pnindent0\pnstart1\pndec{\pntxta.}}
\fi-360\li720\sa200\sl276\slmult1  Pre-compute context totals during training\par
{\pntext\f0 2.\tab} Reuse cumulative distributions where possible\par

\pard\sa200\sl276\slmult1\par
 2. Memory-Efficient Structures\par

\pard 
{\pntext\f0 1.\tab}{\*\pn\pnlvlbody\pnf0\pnindent0\pnstart1\pndec{\pntxta.}}
\fi-360\li720\sa200\sl276\slmult1 Counter instead of dict for automatic zero-handling\par
{\pntext\f0 2.\tab}Tuples for contexts (immutable, hashable, memory-efficient)\par

\pard\sa200\sl276\slmult1\par
3. Early Termination\par

\pard 
{\pntext\f0 1.\tab}{\*\pn\pnlvlbody\pnf0\pnindent0\pnstart1\pndec{\pntxta.}}
\fi-360\li720\sa200\sl276\slmult1  Stop generation at END token\par
{\pntext\f0 2.\tab} Implement max_length safeguard\par

\pard\sa200\sl276\slmult1\par
 4. Batch Operations\par

\pard 
{\pntext\f0 1.\tab}{\*\pn\pnlvlbody\pnf0\pnindent0\pnstart1\pndec{\pntxta.}}
\fi-360\li720\sa200\sl276\slmult1 Process entire text in single pass\par
{\pntext\f0 2.\tab}Vectorized regex operations\par

\pard\sa200\sl276\slmult1\par
\b Testing Strategy\par
\b0  Test coverage:\par

\pard 
{\pntext\f0 1.\tab}{\*\pn\pnlvlbody\pnf0\pnindent0\pnstart1\pndec{\pntxta.}}
\fi-360\li720\sa200\sl276\slmult1 Unit tests for each method (15+ tests)\par
{\pntext\f0 2.\tab}Edge cases (empty text, single word, unicode)\par
{\pntext\f0 3.\tab}Integration tests (end-to-end training/generation)\par
{\pntext\f0 4.\tab}Performance benchmarks\par

\pard\sa200\sl276\slmult1\par
\b Key Test Insights:\par

\pard 
{\pntext\f0 1.\tab}{\*\pn\pnlvlbody\pnf0\pnindent0\pnstart1\pndec{\pntxta.}}
\fi-360\li720\sa200\sl276\slmult1\b0 Reproducibility verified with seed parameter\par
{\pntext\f0 2.\tab}Generation variability confirmed without seed\par
{\pntext\f0 3.\tab}Handles edge cases gracefully (no crashes)\par

\pard\sa200\sl276\slmult1\par
\b Future Improvements\par
\b0 1. Smoothing: Add-k or Kneser-Ney smoothing for better unseen ngram handling\par
2. Backoff:Implement backoff to bigram/unigram for unknown contexts\par
3. Interpolation:Combine trigram, bigram, unigram probabilities\par
4. Caching:Cache frequently accessed probability distributions\par
5. Parallelization:Multi-threaded training for very large corpora\par
\par
\b Conclusion\b0\par
The implementation prioritizes:\par
1. Correctness: True probability distribution sampling\par
2. Efficiency: O(n) training, O(k log m) generation\par
3. Scalability: Linear memory growth with trigrams, not vocabulary\par
4. Maintainability:Clean, well-documented code structure\par
\par
This design achieves production-ready performance while maintaining code clarity and extensibility.\lang9\par
}
 